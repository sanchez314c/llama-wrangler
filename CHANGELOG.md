# Llama Wrangler Changelog

## Version 1.1.0 - October 29, 2025

### üéâ Major Repository Transformation

#### üèóÔ∏è Infrastructure Improvements
- **Updated Dependencies**: Upgraded Electron from 27.0.0 to 39.0.0
- **Enhanced Build System**: Updated electron-builder to 26.0.12
- **Security Upgrades**: Updated electron-context-menu to 4.1.1 and electron-store to 11.0.2
- **Quality Assurance**: Added comprehensive security audit and dependency analysis scripts

#### üìö Documentation Enhancement
- **New Documentation Suite**: Created comprehensive documentation structure
  - `docs/DEVELOPMENT.md` - Complete development setup and architecture guide
  - `docs/SECURITY_ASSESSMENT.md` - Detailed security analysis and recommendations
  - `docs/DOCUMENTATION_INDEX.md` - Central documentation navigation
  - `CLAUDE.md` - AI assistant development guidance
- **Enhanced GitHub Templates**: Improved issue and pull request templates
- **Professional README**: Updated with better structure and navigation

#### üîß Development Workflow
- **CI/CD Pipeline**: Added automated quality checks with GitHub Actions
- **Security Monitoring**: Implemented automated vulnerability scanning
- **Build Optimization**: Enhanced build scripts with better error handling
- **Testing Framework**: Added quality assurance checkpoints

#### üõ°Ô∏è Security Improvements
- **Security Audit**: Comprehensive dependency vulnerability assessment
- **Quality Checks**: Automated security scanning in CI/CD pipeline
- **Best Practices**: Security guidelines and implementation patterns
- **Risk Assessment**: Detailed security mitigation strategies

#### üìÅ Project Organization
- **Optimized .gitignore**: Comprehensive ignore patterns for all file types
- **Professional Structure**: Reorganized directories for better maintainability
- **Build Resources**: Dedicated directories for icons and assets
- **Testing Structure**: Organized test directories for unit and integration tests

#### üîç Quality Assurance
- **Bloat Analysis**: Enhanced dependency size monitoring
- **Performance Monitoring**: Added build-time performance checks
- **Documentation Validation**: Automated documentation quality checks
- **Code Quality**: Implemented code review standards

## Version 1.0.0 - July 27, 2025

### Initial Setup
- Reviewed and organized project for GitHub standards
- Created LICENSE (MIT), .gitignore, README.md, CONTRIBUTING.md
- Renamed files to remove 'llama-wrangler-' prefix
- Created runWRANGLER.sh start script
- Set up build configuration for standalone binaries

### First Build
- Built Mac binary successfully (Intel and Apple Silicon)
- App opened but showed "llama.cpp not found" error
- User's llama.cpp is at custom location: /.metallama.cpp

### Path Selection Feature Added
- Added dialog to select existing llama.cpp directory
- Implemented persistent path storage using electron-store
- Updated to check multiple model directories
- Fixed to look in user's actual directory: /Users/heathen-admin/.METALlama.cpp

### Model Display Issues Fixed
- App now finds models in llama.cpp directory
- Shows model location [llama.cpp] vs [Wrangler]
- Added "Load Model" button for each model
- Shows "Currently Active" indicator

### Web View Issues
- HuggingFace/Ollama tabs were blank (webview issue)
- Attempted iframe solution
- Eventually replaced with instruction pages

### Critical Issue: Model Switching Broke User's Setup
- User has sophisticated LaunchAgent setup (com.llama.mps.server)
- System uses preference file: ~/.config/llama_mps_server/preferred_model
- Initial implementation killed server process, breaking Docker connection
- Attempted to fix by using LaunchAgent restart but created more issues

### Bug Fixes - Latest Session
**Fixed Issues:**
1. **Model Switching Verification Error**: 
   - Fixed "Failed to verify model switch" error
   - Added retry logic with flexible model name matching
   - Now returns success even if verification partially fails since LaunchAgent handles it
   
2. **Download Function Errors**:
   - Fixed "Error invoking remote method 'download-huggingface'" 
   - Added proper error handling for missing dependencies
   - Fixed Ollama download to check if Ollama is installed first
   - Updated script path to use correct location (download_hf.py in root)

3. **HuggingFace and Ollama Tab URLs**:
   - Fixed HuggingFace tab to show https://huggingface.co/models
   - Fixed Ollama tab to show https://ollama.com/search
   - Added proper sandbox permissions for navigation

4. **Model Switch Display**:
   - Improved model verification after switching
   - Added delay to allow server to fully load new model
   - Fixed status display refresh after model changes

## Latest Session (July 27, 2025 Evening) - Major Improvements

### ‚úÖ Completed Features
1. **Fixed Ollama Page Contrast** - Dark theme compatibility with CSS injection
2. **Added "Add HuggingFace Model" Button** - Blue button that grabs current URL and auto-downloads
3. **Added "Add Ollama Model" Button** - Orange button that extracts model name and auto-downloads
4. **Fixed Download Crash** - App no longer crashes when downloading models without GGUF files
5. **Improved Error Handling** - Clean error messages instead of technical stderr output
6. **Enhanced Download Script** - Complete rewrite with architecture detection and progress tracking
7. **Fixed Model Directory Issues** - Models now automatically copied to MetalLlama directory for proper switching

### üö® Critical Issues Resolved
- **Server Restart Problems**: Improved LaunchAgent restart logic using `launchctl kickstart`
- **Corrupted Model Detection**: Fixed model file corruption during copy operations
- **Model Switching Restrictions**: Removed "not usable for switching" limitations
- **Download Progress**: Implemented accurate percentage tracking for all download/conversion steps

### ‚ö†Ô∏è Current Issues
- **Download Buttons Missing**: The "Add HuggingFace Model" and "Add Ollama Model" buttons are not visible in the app despite being in the code
- **Server Restart Fragility**: LaunchAgent restart process still causes temporary server outages

### Current State
- MetalLlama.cpp server: ‚úÖ Running (PID 21232, health check passes)
- OpenWebUI connectivity: ‚úÖ Working at host.docker.internal:7070  
- Model switching: ‚úÖ Working with automatic MetalLlama directory integration
- Download functionality: ‚úÖ Enhanced with progress tracking and architecture detection
- UI buttons: ‚ùå Missing download buttons (needs investigation)

### Key Technical Details
- Enhanced download_hf.py with ModelConverter class and progress tracking
- Automatic model architecture detection (Llama, Mistral, Qwen, Gemma, etc.)
- Pre-converted GGUF file detection and direct download when available
- Automatic model conversion and quantization when GGUF files don't exist
- Smart model placement in MetalLlama directory for LaunchAgent compatibility

## Session - July 28, 2025 - Opus/Sonnet Transition & Major Features

### Added Features
1. **Auto-delete duplicate models** - After copying to MetalLlama, deletes from Wrangler directory
2. **Standard title bar** - Removed hiddenInset style for easier window dragging
3. **Quantization UI** - Complete dialog with all quantization levels (Q2_K through Q8_0)
4. **Quantize button** - Blue button on each model between Load and Delete
5. **Fixed Ollama downloads** - Restored download_ollama.py script

### Fixed Issues
1. **App stability** - Fixed crashes by reverting model directory filtering
2. **Console.log crashes** - Removed debug statements that caused instability
3. **Window dragging** - Added standard title bar back
4. **Ollama model downloads** - Now working properly

### Current Working Features
- ‚úÖ Download models from HuggingFace (auto-converts to GGUF if needed)
- ‚úÖ Download models from Ollama registry
- ‚úÖ Switch models using LaunchAgent
- ‚úÖ Delete models
- ‚úÖ Show model size, type, and location
- ‚úÖ Visual indicator for currently active model
- ‚úÖ Standard title bar for easy window movement

### Known Issues
- ‚ö†Ô∏è Quantization crashes app when used (UI works, backend handler needs fixing)
- ‚ö†Ô∏è Auto-delete not consistent for Ollama downloads
- ‚ö†Ô∏è Directory confusion between root and _opusversion (resolved by removing _opusversion)

### Technical Notes
- Working directory: /Users/heathen-admin/Development/Projects/ACTIVE-LlamaWrangler
- Removed _opusversion directory to eliminate confusion
- Quantization uses llama-quantize at /Users/heathen-admin/.METALlama.cpp/build/bin/llama-quantize
- App is stable when quantization is disabled

## Session - July 29, 2025 - Critical Fixes & Auto-Conversion Feature

### Fixed Critical Issues
1. **App crashes after startup** - Removed ALL console.log statements from main.js and renderer.js
2. **Quantization crashes** - Fixed library path issue by setting DYLD_LIBRARY_PATH environment variable
3. **Download crashes** - Fixed variable naming conflicts (process ‚Üí downloadProcess/quantizeProcess)
4. **Variable naming bugs** - Fixed all spawn process variable shadowing issues

### New Feature: Auto-Conversion
- **Auto-download and convert**: When no Q4_K_M/Q4_K GGUF files exist, automatically:
  - Downloads base model from HuggingFace
  - Converts to GGUF format using llama.cpp
  - Quantizes to requested level (default Q4_K_M)
  - Cleans up temporary files
- **Better error handling**: Improved messages for large model downloads
- **Memory optimization**: Limited concurrent downloads, excluded unnecessary file types

### Current Status
- ‚úÖ App runs without crashing
- ‚úÖ Downloads work (both pre-quantized and base models)
- ‚úÖ Quantization works properly
- ‚úÖ Auto-conversion for models without GGUF files
- ‚ö†Ô∏è App may close after extended idle time
- ‚ö†Ô∏è Large model downloads may still cause issues

### Key Fixes Applied
1. Removed all console.log/console.error statements causing crashes
2. Fixed process variable shadowing in download handlers
3. Fixed quantization library path with DYLD_LIBRARY_PATH
4. Added auto-conversion capability to download_hf.py
5. Improved memory management for large downloads